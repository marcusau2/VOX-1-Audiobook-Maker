# VOX-1 AUDIOBOOK MAKER - SESSION HANDOFF
**Date:** 2026-01-26
**Status:** üß™ TESTING IN PROGRESS - Batch Size Limits

---

## CURRENT STATUS

### What's Being Tested Right Now
- User is testing batch 3 on RTX 4090 (24GB) with "Lords of the Sith" audiobook
- Rendering 20 chapters (61+ chunks in Chapter 2)
- Waiting for activity log results to see if generation completes without freezing

### Critical Discovery: Qwen3-TTS Batch Processing Limits

**VRAM allocation stays stuck at ~2.06GB** even with 24GB available
- Reserved memory: 2.15GB
- Allocated memory: 2.06GB (doesn't grow during batch processing)
- Previous attempts with batch 5 froze/stalled on RTX 4090

**Official Qwen3-TTS Documentation:**
- Technical report tests concurrency levels: 1, 3, 6
- High per-chunk VRAM requirements (~5-6GB per batch item)
- Batch processing IS intended for multiple text chunks (confirmed correct usage)
- No explicit maximum documented, but testing shows limits exist

**User Testing Results:**
- Portable version: Batch 3 worked, anything higher failed (even on 4090)
- Current version: Testing batch 3 now
- Hypothesis: Model architecture limits, not PyTorch allocator issue

---

## WHAT WAS DONE THIS SESSION

### 1. Investigated VRAM Allocation Issue ‚ùå (Reverted)

**Attempted Fix (Commit cb5045b):**
- Added `PYTORCH_CUDA_ALLOC_CONF` settings (expandable_segments, max_split_size_mb:512)
- Set tiered `set_per_process_memory_fraction` (95%/90%/85%)
- Added GPU warmup step before rendering
- Re-enabled cuDNN benchmarking

**Result:** Didn't help - generation still stalled at batch 5

**Learned:**
- Reserved vs Allocated memory discrepancy is NORMAL PyTorch behavior
- `set_per_process_memory_fraction()` doesn't control Task Manager numbers
- The "fixes" were addressing the wrong problem

### 2. Reverted to Portable Version Config ‚úÖ (Commit c0af26e)

**What We Did:**
- Restored simple CUDA configuration from commit 21bdd0a (portable version)
- Removed all "aggressive" VRAM allocation attempts
- Kept only: `torch.backends.cudnn.benchmark = False`
- Kept models caching to project folder (good fix)

**New Conservative Batch Size Limits:**
```python
24GB GPU (4090):     batch 3  # Tested, proven stable in portable version
12-16GB GPU (4070):  batch 2  # Safe for mid-range
8-10GB GPU:          batch 1  # Single batch only
```

### 3. Updated Advanced Settings UI ‚úÖ (Commit 2a4c04a)

**Changes to app.py:**
- Batch slider: 1-32 ‚Üí 1-3 (only allows tested values)
- Default batch: 5 ‚Üí 2 (safe for most GPUs)
- Updated info text to explain conservative limits
- Updated all default references in code

**Users can now only select batch sizes that are proven stable**

### 4. Created Update Mechanism ‚úÖ (Commit 11f7553)

**New Files:**
- `UPDATE-VOX-1.bat` - Launcher
- `Update.ps1` - PowerShell script with full update logic

**Features:**
- Auto-detects Git installation
- Shows changelog before updating
- Detects if requirements.txt changed
- Automatically reinstalls dependencies if needed
- Provides manual instructions for ZIP downloads

### 5. Fixed Documentation ‚úÖ (Commit 86be09d)

- Changed all "Qwen2-TTS" references to "Qwen3-TTS"
- Updated README.md and MANUAL_INSTALL.md

---

## BACKUP BRANCHES AVAILABLE

Two safety nets created for easy reverting:

1. **backup-before-vram-fix** (commit d26fff1)
   - Before any VRAM allocation experiments
   - Portable version + initial optimizations

2. **backup-before-batch-fix** (commit 11f7553)
   - Before reverting VRAM fixes
   - Has all the attempted VRAM allocation changes

**To revert:**
```bash
git checkout backup-before-batch-fix   # Revert to before batch changes
git checkout backup-before-vram-fix    # Revert to before VRAM experiments
git checkout main                      # Return to latest
```

---

## CURRENT CODE STATE

### Latest Commits on Main:
```
2a4c04a - Update Advanced Settings UI for conservative batch limits
c0af26e - Revert to portable version CUDA config + conservative batch limits
11f7553 - Refactor update script to use PowerShell (like installer)
86be09d - Fix incorrect Qwen2-TTS references to Qwen3-TTS
cb5045b - Fix VRAM allocation issue and add update mechanism (REVERTED)
```

### backend.py - Key Settings:
```python
# Line 233: Simple CUDA config (like portable version)
torch.backends.cudnn.benchmark = False

# Lines 286-302: Conservative batch size recommendations
def _suggest_batch_size(self, total_vram_gb):
    if total_vram_gb >= 22:  return 3  # 4090
    elif total_vram_gb >= 11: return 2  # 4070 Ti
    elif total_vram_gb >= 7:  return 1  # 3070
    else: return 1
```

### app.py - UI Settings:
```python
# Line 59: Default batch size
"batch_size": 2

# Line 488: Slider range
self.batch_slider = ctk.CTkSlider(batch_frame, from_=1, to=3, number_of_steps=2)
```

---

## KNOWN ISSUES

### 1. Vestigial Batch Size Warning (Minor)
**Line 288 in backend.py:**
```
NOTE: For {total_vram_gb}GB VRAM, recommended batch size is {recommended_batch} (current: {self.batch_size})
```

This message compares recommended vs current batch size, but is now confusing since we capped everything at 3. Should be removed or simplified once testing confirms optimal values.

### 2. VRAM Not Growing During Generation (Under Investigation)
**Observed behavior:**
- Allocated memory stays at ~2.06GB throughout rendering
- Reserved memory stays at ~2.15GB
- Task Manager shows matching numbers
- This is likely normal PyTorch behavior, not a bug

**Questions for next session:**
- Does generation complete without freezing at batch 3?
- Does VRAM allocation change during larger chapter rendering?
- Is batch 2 or batch 1 more stable?

### 3. First-Time User Experience
**Current flow:**
- New users get batch 2 by default (conservative)
- Auto-detect suggests batch based on GPU VRAM
- May want to add onboarding tooltip explaining batch limits

---

## TESTING NEEDED

### Priority 1: Confirm Batch 3 on RTX 4090
**In Progress:**
- User testing with "Lords of the Sith" (20 chapters, 61+ chunks)
- Need activity log from Chapter 2+ to see:
  - Does it complete without freezing?
  - What are VRAM numbers during larger batches?
  - Performance metrics (seconds/chunk)

### Priority 2: Test on RTX 4070 Ti (12GB)
- Should test batch 2 (recommended)
- Try batch 3 to see if it works
- Verify no OOM errors

### Priority 3: Verify Update Script Works
- Test UPDATE-VOX-1.bat on both PCs
- Confirm it detects updates
- Verify dependency reinstallation works

---

## NEXT SESSION PRIORITIES

### If Batch 3 Works on 4090:
1. Remove vestigial batch warning (backend.py line 288)
2. Update documentation with confirmed batch limits
3. Test on 4070 Ti (12GB) with batch 2
4. Consider batch 3 as safe maximum for all GPUs

### If Batch 3 Still Fails:
1. Test batch 2 on 4090
2. Test batch 1 as fallback
3. May need to cap at batch 2 maximum for all GPUs
4. Investigate if model-specific (0.6B vs 1.7B) limits exist

### General Cleanup:
1. Remove vestigial comparison message
2. Update PERFORMANCE_OPTIMIZATIONS.md with batch findings
3. Add batch size guidance to USER_GUIDE.txt
4. Consider adding VRAM monitoring graph (future enhancement)

---

## IMPORTANT RESEARCH FINDINGS

### Qwen3-TTS Official Documentation
- GitHub: https://github.com/QwenLM/Qwen3-TTS
- Technical Report: https://arxiv.org/html/2601.15621v1
- Batch processing confirmed as intended use for multiple text chunks
- `generate_voice_clone()` accepts list of strings: `text=["Chunk 1", "Chunk 2"]`
- Official testing shows concurrency 1, 3, 6 in technical report

### PyTorch CUDA Memory Management
- Reserved vs Allocated memory discrepancy is normal behavior
- `set_per_process_memory_fraction()` is NOT a hard limit on nvidia-smi
- PYTORCH_CUDA_ALLOC_CONF helps with fragmentation but didn't solve our issue
- RTX 4090 has known compatibility quirks with PyTorch batch processing

**Key Sources:**
- PyTorch CUDA Caching Allocator: https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html
- CUDA semantics docs: https://docs.pytorch.org/docs/stable/notes/cuda.html
- RTX 4090 PyTorch issues: https://discuss.pytorch.org/t/rtx4090-issue-with-pytorch/188653

---

## FILE LOCATIONS

### Core Files:
- `backend.py` - TTS engine (1,714 lines)
- `app.py` - GUI frontend (1,351+ lines)
- `booksmith_module/` - EPUB/PDF processing
- `UPDATE-VOX-1.bat` + `Update.ps1` - Update system
- `Install-VOX-1.bat` + `Setup.ps1` - Installer

### Documentation:
- `README.md` - Main docs with installation
- `USER_GUIDE.txt` - Complete user manual
- `MANUAL_INSTALL.md` - Advanced installation
- `PERFORMANCE_OPTIMIZATIONS.md` - Technical optimization details
- `VRAM_FIX_INFO.txt` - Documentation of failed VRAM fixes (can delete)
- `Handoff.txt` - This file

### User Data (Preserved During Updates):
- `Output/` - Generated audio files
- `VOX-Output/` - Master voices
- `models/` - Cached AI models (8-10GB)
- `user_settings.json` - User preferences
- `temp_work/` - Chunk cache for resume

---

## QUICK COMMAND REFERENCE

### Test Installation:
```bash
Install-VOX-1.bat          # Install/reinstall dependencies
RUN-VOX-1.bat              # Launch app
Launch-Debug.bat           # Launch with console for debugging
```

### Update System:
```bash
UPDATE-VOX-1.bat           # Check for and apply updates
```

### Git Commands:
```bash
git status                 # Check current state
git log --oneline -10      # Show recent commits
git checkout <branch>      # Switch branches
git checkout main          # Return to latest
```

### Revert if Needed:
```bash
git checkout backup-before-batch-fix     # Revert batch changes
git checkout backup-before-vram-fix      # Revert VRAM experiments
git checkout main                        # Back to latest
```

---

## USER ENVIRONMENT

### PC 1: Development/Testing
- GPU: RTX 4070 Ti (12GB VRAM)
- Testing batch 2 recommended
- Installation method: Git clone

### PC 2: High-End Testing
- GPU: RTX 4090 (24GB VRAM)
- Currently testing batch 3
- Installation method: Git clone
- Running "Lords of the Sith" render test

### System Requirements:
- Windows 10/11 64-bit
- NVIDIA GPU with 8GB+ VRAM
- Python 3.10 (auto-installed)
- PyTorch 2.7.1 with CUDA 12.8 (auto-installed)

---

## WHAT USER NEEDS TO PROVIDE NEXT SESSION

1. **Complete activity log from Chapter 2+ rendering**
   - Shows if batch 3 completes without freezing
   - VRAM allocation numbers during processing
   - Performance metrics (seconds/chunk)

2. **Test results from 4070 Ti (if time permits)**
   - Batch 2 performance
   - Whether batch 3 works on 12GB GPU

3. **Any error messages or unexpected behavior**

---

## SUCCESS CRITERIA

### Must Work:
- ‚úÖ Update mechanism (UPDATE-VOX-1.bat)
- ‚úÖ Conservative batch limits (1-3)
- ‚úÖ Simple CUDA config (portable version)
- ‚è≥ Generation completes without freezing (testing)

### Should Work:
- ‚è≥ Batch 3 on 24GB GPU (4090) - testing now
- ‚è≥ Batch 2 on 12GB GPU (4070 Ti) - needs testing
- ‚úÖ Models cache to project folder
- ‚úÖ All UI reflects correct limits

### Nice to Have:
- Better VRAM monitoring/visualization
- Onboarding tooltips for batch settings
- Performance comparison data

---

## LESSONS LEARNED THIS SESSION

1. **Don't over-engineer memory management** - PyTorch's default behavior is often correct
2. **Research official documentation first** - Saved time understanding batch usage
3. **Conservative limits beat aggressive optimization** - Stability > speed
4. **Always create backup branches** - Easy reverting is crucial
5. **Task Manager numbers can be misleading** - Reserved ‚â† Allocated is normal

---

## STATUS SUMMARY

‚úÖ **Completed:**
- Reverted to stable portable version configuration
- Applied conservative batch limits based on testing
- Updated UI to prevent users from selecting unstable values
- Created robust update mechanism
- Fixed documentation errors
- Created backup branches for safety

‚è≥ **In Progress:**
- Testing batch 3 on RTX 4090 (24GB)
- Waiting for activity log results

üìã **Next Steps:**
- Analyze test results from batch 3 rendering
- Remove vestigial batch warning message
- Test on 4070 Ti with batch 2
- Update documentation with confirmed limits

---

**Last Updated:** 2026-01-26
**Current Commit:** 2a4c04a
**Git Branch:** main
**Status:** Ready for testing validation

**Backup Branches:**
- backup-before-vram-fix
- backup-before-batch-fix

---

END OF HANDOFF
